# ğŸ”­ Overview â€“ AI Gesture Control for 3D Workflows

**Description:**
MotionCoder turns **sensor streams** into **semantic gestures/intents** in real time, then routes them to your **target software** via lightweight connectors.
The stack is **modular** (Sensors â†’ AI Interpretation â†’ Connectors), **low-latency**, and optimized for **CAD/DCC**â€”yet portable to many other domains.

## ğŸ¥ Layer 1 â€“ Sensor I/O

| ğŸ§© **Module**  | ğŸ“ **Short Description**                                                                                                                                                         | ğŸ”Œ **Hardware / Deps**                                                    | âš–ï¸ **License** | âš ï¸ **Notes**                                                             | ğŸš¦ **Status**         | ğŸ”— **Link** |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- | -------------- | ------------------------------------------------------------------------ | --------------------- | ----------- |
| **Pi5Track3D** | **RAW10 mono ingest** auf Pi 5; GPU-/NEON-optimierte Preproc (undistort/normalize), optional On-Pi Keypoints; Stream an Host fÃ¼r Triangulation.                                  | **Raspberry Pi 5** (4/8 GB), 2Ã— MIPI-CSI (OV9281 o. Ã¤.), NVMe/USB-2.5 GbE | Apache-2.0     | Fix exposure/gain; HW-Trigger empfohlen; Jumbo MTU fÃ¼r Streaming.        | ğŸŸ¡ Planned            | coming soon |
| **MVCore3D**   | **MJPEG ingest** (libjpeg-turbo), Triangulation via **Anipose**, 2D-Keypoints via **MMPose**; **CPU-tuned** Plug-&-Play fÃ¼r Multi-Cam.                                           | USB-UVC Cams (auch Low-Cost), optional Sync                               | Apache-2.0     | HÃ¶here CPU-Last durch Decode; deterministisch bei festen Auto-Settings.  | ğŸŸ  later            | coming soon |
| **MVYUV3D**    | **Uncompressed YUV ingest** (**YUY2/UYVY 4:2:2**, optional **NV12 4:2:0**); geringere CPU-Last als MJPEG, hÃ¶here USB-Last als MONO8; Triangulation **Anipose**, Pose **MMPose**. | UVC-Cams mit YUV-Output; stabile Beleuchtung; Soft/Hard-Sync optional     | Apache-2.0     | **libjpeg-turbo** nicht nÃ¶tig; Auto-Funktionen aus â‡’ mehr Determinismus. | ğŸŸ  later            | coming soon |
| **MVRaw3D**    | **RAW10/12 (Bayer/Mono) ingest**, Debayer/Denoise-Pipeline; **niedrige Latenz & hohe Fidelity** gegenÃ¼ber MJPEG; GPU-Beschleunigung wo mÃ¶glich.                                  | Global-/Rolling-Shutter Cams; **HW-Sync empfohlen**                       | Apache-2.0     | Exakte Lens-/Noise-Parameter verbessern Keypoints deutlich.              | ğŸŸ  later            | coming soon |
| **Leap2Pose**  | **LeapC ingestion â†’ normalisierte Posen/Streams** (einheitliche Joints/Units) fÃ¼r Downstream-Adapter.                                                                            | Leap Motion Controller 1/2                                                | MIT            | Desktop-first; VR optional.                                              | ğŸŸ¢ Active             | coming soon |
| **MVMono3D**   | **Synchrones Mono-Multi-Cam** (Global Shutter) mit hoher Okklusions-Robustheit & PrÃ¤zision; NIR-tauglich.                                                                        | 3â€“4 Mono-Cams, **IR + Bandpass**, **HW-Trigger/Sync**                     | Apache-2.0     | Kurze Belichtung (0.3â€“0.8 ms), fester Gain.                              | ğŸŸ  Targeted next year | coming soon |
| **TDMStrobe**  | **Time-Division-Multiplexed IR-Strobe & Camera-Trigger** (Phasensteuerung A/B/C/D) fÃ¼r MV*-Pipelines.                                                                            | IR-LED/VCSEL Arrays, MOSFET-Treiber, MCU                                  | Apache-2.0     | Reduziert IR-Crosstalk; **850 nm Bandpass** empfohlen.                   | ğŸŸ  later            | coming soon |




## ğŸ§  Layer 2 â€“ AI Interpretation (Pose â†’ Intents/Gestures)

| ğŸ§© **Module**        | ğŸ“ **Short Description**                                 | ğŸ” **I/O**                  | âš–ï¸ **License** | âš ï¸ **Notes** | ğŸš¦ **Status**  | ğŸ”— **Link**                                                                   |
| -------------------- | --------------------------------------------------------- | ---------------------------- | -------------- | ------------ | -------------- | ------------------------------------------------------------------------------ |
| **MotionCoder**      | Real-time gestures/intents, state machine, context logic. | Poses/keypoints from Layer 1 | Apache-2.0     | â€”            | ğŸŸ¡ In progress | [MotionCoder](https://github.com/xtanai/motioncoder) |
| **Pose2Gizmo**       | Poses â†’ 3D manipulator/gizmo commands & visualization.    | Normalized poses, tool hints | MIT            | â€”            | ğŸŸ¡ Planned     | coming soon   |

## ğŸ”— Layer 3 â€“ Connectors (DCC/CAD/Engines)

| ğŸ§© **Module**           | ğŸ“ **Short Description**                               | ğŸ–¥ï¸ **Target System** | âš–ï¸ **License** | âš ï¸ **Notes** | ğŸš¦ **Status**                 | ğŸ”— **Link**                                                                         |
| ------------------------ | ------------------------------------------------------ | --------------------- | -------------- | ------------ | ----------------------------- | ------------------------------------------------------------------------------------ |
| **Coder2Blender**        | Add-on/API bridge: gestures â†’ operators/hotkeys/nodes. | Blender               | MIT            | â€”            | ğŸŸ¡ Research (API exploration) | coming soon  |
| **Coder2Unreal**         | Plugin/bridge: gestures â†’ Blueprint/C++ events.        | Unreal Engine         | MIT            | â€”            | ğŸŸ¡ Planned                    | coming soon   |
| **Coder2Dassault**       | Macro/API bridge: gestures â†’ CAD commands.             | Dassault (SWX/CATIA)  | MIT            | â€”            | ğŸŸ  Targeted for next year     | coming soon |


**Why separate (MotionCoder â†” Coder2$)?**

* **Modularity:** MotionCoder (detection/semantics) remains independent of any target application.
* **Reuse:** One semantics engine, many **Coder2$** adapters (CAD, DCC, and beyond).
* **Breadth:** **100+ potential software targets** (integrations), far beyond the examples shown here.
* **Maintainability & Scale:** Target-API changes impact only the relevant **Coder2$**, not the core.
* **Portability:** Faster rollout to **new applications**â€”e.g., sign-language assistanceâ€”beyond CAD/DCC.



## ğŸ¥ Recommended Sensors

Choose from three sensor modules â€” **MVCore3D**, **Leap2Pose**, and **MVMono3D** â€” based on your **budget** and target **quality**, with matching **hardware recommendations**. More in the GitHub docs: ğŸ‘‰ [Sensor Guide](https://github.com/xtanai/sensor-guide)


## ğŸ—ºï¸ Roadmap

Coming soon. Iâ€™m still in the research phase. ğŸš€


## â“ FAQ (Top 10)

#### 1) Can I use a VR headset?
Yes, you can try VR, but the project is **optimized for desktop workflows**. VR is optional and not fully implemented yet; support may be added later.

#### 2) Can 'Meta Quest 3' do 3D hand tracking?
Yes, you can try it, but itâ€™s **not optimal for CAD-grade precision**. Inside-out tracking and 2Dâ†’3D lifting are fine for previews/VR UX, but suffer from **occlusion**, **drift**, **no hardware sync/trigger**, and **higher latency**.  
MotionCoder prioritizes **deterministic multi-view 3D capture** (global-shutter/NIR) with **low-latency semantics** for reliable CAD/DCC work.

#### 3) What about LiDAR and ToF sensors?
Useful in constrained scenes, but often limited by **noise**, **low hand resolution**, **latency**, and **vendor lock-in**. We focus on **global-shutter/NIR multi-view** for precision.

#### 4) Why multiple cameras instead of one?
Single-view suffers from **occlusion** and **scale/pose ambiguity**. Multi-view provides **stable 3D geometry**, **less drift**, and **higher intent confidence**â€”critical for CAD/DCC.

#### 5) Do I need markers or a glove?
Noâ€”tracking is **markerless**. Optional aids (e.g., a **2D touch panel** or fiducials) can boost **fine precision** in special setups, but arenâ€™t required.

#### 6) Do I need sign-language skills?
No. Core gestures are **beginner-friendly**. Sign-language nuances are used to improve **expressiveness and disambiguation**, not as a requirement.

#### 7) Privacy & security?
Everything runs **on-premises**. No cloud required. We follow **data minimization**; logs/frames can be disabled or anonymized.

#### 8) Supported apps?
Starting with **Blender** (adapter). Next targets: **Unreal, Unity, Maya, SolidWorks, Rhino/NX**, and other DCC tools. Adapters are modular (`Coder2$`).

#### 9) Will high-quality hardware become cheap?
Costs are trending down. We expect **consumer-priced, higher-quality options** over time, but timelines depend on vendors and supplyâ€”no hard promises.

#### 10) Mouse & keyboard?
They stay. Habits donâ€™t change overnight. Cameras mount neatly to the monitor/frame and **donâ€™t get in the way**â€”MotionCoder **augments** existing input, it doesnâ€™t replace it.


